<?xml version="1.0" encoding="UTF-8"?>
<cherrytree>
  <node name="Attacking NLP" unique_id="3" prog_lang="custom-colors" tags="" readonly="0" nosearch_me="0" nosearch_ch="0" custom_icon_id="0" is_bold="0" foreground="" ts_creation="1702644999" ts_lastsave="1702645327">
    <rich_text scale="h3" weight="heavy">Prompt Injection</rich_text>
    <rich_text>

Prompt injection attacks manipulate a chatbot's responses by inserting specific queries, tricking it into unexpected reactions.
These attacks could range from extracting sensitive info to spewing out misleading responses.
</rich_text>
    <rich_text weight="heavy">
Attack Example:</rich_text>
    <rich_text>


</rich_text>
    <rich_text weight="heavy">
Reason of Attacks:</rich_text>
    <rich_text>

The root of the issue often lies in how chatbots are trained. They learn from vast datasets, ingesting tons of text to understand and mimic human language.
The quality and the nature of the data they are trained on deeply influence their responses.
For instance, a chatbot trained on corporate data might inadvertently leak sensitive information when prodded.
</rich_text>
    <encoded_png char_offset="258" justification="left" link="" sha256sum="d9ba73022fd394741f6dd1201210c90d871219b0345931baaf786bfcfac1e0b9"/>
  </node>
</cherrytree>
